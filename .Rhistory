DTHLt <- table(predictions= DTHLpredTest, actual = DTHLactualTest)
DTHLt # Confusion matrix
DTHLaccuracy <- sum(diag(DTHLt))/sum(DTHLt)
DTHLaccuracy
## Visualization of probabilities
hist(DTHLprobTest[,2], breaks = 100)
## ROC and Area Under the Curve
DTHLROC <- roc(DTHLactualTest, DTHLprobTest[,2])
plot(DTHLROC, col="blue")
DTHLAUC <- auc(DTHLROC)
DTHLAUC
#A new dataframe with Predicted Prob, Actual Value and Predicted Value
DTHLpredicted_data <- data.frame(Probs = DTHLprobTest, Actual_Value= DTHLactualTest ,Predicted_Value = DTHLpredTest )  #Create data frame with prob and predictions
DTHLpredicted_data <- DTHLpredicted_data[order(DTHLpredicted_data$Probs.1, decreasing=TRUE),] # Sort on Probabilities
DTHLpredicted_data$Rank <- 1:nrow(DTHLpredicted_data) # Add a new variable rank
ggplot(data=DTpredicted_data, aes(x=Rank, y=Probs.1)) +
geom_point(aes(color = DTHLpredicted_data$Actual_Value)) + xlab("Index") + ylab("Predicted Probability of Heating Load")
DTHLpredProbability <-predict(DTHLmodel, DTHLnewData, type='prob')
DTHLpredProbability
## Step 7 - EXAMINING STABILITY - Creating Decile Plots for Class 1 or 0 Sort
#
#-----Create empty df-------
DTdecile<- data.frame(matrix(ncol=4,nrow = 0))
colnames(DTdecile)<- c("Decile","per_correct_preds","No_correct_Preds","cum_preds")
#-----Initialize variables
DTnum_of_deciles = 10
DTObs_per_decile <- nrow(DTpredicted_data)/DTnum_of_deciles
DTdecile_count = 1
DTstart = 1
DTstop = (DTstart-1) + DTObs_per_decile
DTprev_cum_pred <- 0
DTx = 0
#-----Loop through DF and create deciles
while (DTx < nrow(DTpredicted_data)) {
DTsubset <- DTpredicted_data[c(DTstart:DTstop),]
DTcorrect_count <- ifelse(DTsubset$Actual_Value == DTsubset$Predicted_Value, 1, 0)
DTno_correct_Preds <- sum(DTcorrect_count, na.rm = TRUE)
DTper_correct_Preds <- (DTno_correct_Preds / DTObs_per_decile) * 100
DTcum_preds <- DTno_correct_Preds+DTprev_cum_pred
DTaddRow <- data.frame("Decile" = DTdecile_count, "per_correct_preds" = DTper_correct_Preds, "No_correct_Preds" = DTno_correct_Preds, "cum_preds" = DTcum_preds)
DTdecile <- rbind(DTdecile,DTaddRow)
DTprev_cum_pred <- DTprev_cum_pred+DTno_correct_Preds
DTstart <- DTstop + 1
DTstop = (DTstart - 1) + DTObs_per_decile
DTx <- DTx+DTObs_per_decile
DTdecile_count <- DTdecile_count + 1
}
#------Stability plot (correct preds per decile)
plot(DTdecile$Decile,DTdecile$per_correct_preds,type = "l",xlab = "Decile",ylab = "Percentage of correct predictions",main="Stability Plot for Class 1")
# Clear data
rm(list = ls())  # Removes all objects from the environment
# Clear packages
p_unload(all)    # Remove all contributed packages
# Clear plots
graphics.off()   # Clears plots, closes all graphics devices
# Clear console
cat("\014")      # Mimics ctrl+L
# Clear R
# Clear R
#   You may want to use Session > Restart R, as well, which
# Clear R
#   You may want to use Session > Restart R, as well, which
#   resets changed options, relative paths, dependencies,
# Title:  Decision Trees
# File:   ENB2012_data.R
# Course: Knowledge Discovery and Data Analytics I - COMP 6115
# INSTALL AND LOAD PACKAGES ################################
# Install pacman if you don't have it (uncomment next line)
# install.packages("pacman")
# Install and/or load packages with pacman
pacman::p_load(  # Use p_load function from pacman
GGally,        # Plotting option
magrittr,      # Pipes
pacman,        # Load/unload packages
rio,           # Import/export data
tidyverse      # So many reasons
)
# LOAD AND PREPARE DATA ####################################
# Import the data
df <-  import("data/ENB2012_data.csv") %>%
as_tibble()  # Save as tibble, which prints better
# Look at the variable names
df %>% names()
head(df)
summary(df)
class(df)
str(df)
boxplot(df$Y1)
hist(df$Y1)
# Rename the class label as y; change values 0 to "notSpam"
# and 1 to "spam"; convert to factor
df %<>%
rename(y = Y1) %>%   # Rename class variable as `y`
mutate(
y = ifelse(
y >= 30,
"High",
"Low"
)
) %>%
mutate(y = factor(y))  # Recode class label as factor
# Check the variable `y`; `forcats::fct_count` gives
# frequencies in factor order
df %>%
pull(y) %>%  # Return a vector instead of a dataframe
fct_count()  # Count frequencies in factor order
# SPLIT DATA ##############################################
# Some demonstrations will use separate testing and training
# datasets for validation.
# Set random seed for reproducibility in processes like
# splitting the data
set.seed(1)  # You can use any number here
# Split data into training (trn) and testing (tst) sets
df %<>% mutate(ID = row_number())  # Add row ID
trn <- df %>%                      # Create trn
slice_sample(prop = .70)         # 70% in trn
tst <- df %>%                      # Create tst
anti_join(trn, by = "ID") %>%    # Remaining data in tst
select(-ID)                      # Remove id from tst
trn %<>% select(-ID)               # Remove id from trn
df %<>% select(-ID)                # Remove id from df
# EXPLORE TRAINING DATA ####################################
# Bar chart of `y`, which is the spam/not class variable
trn %>%
ggplot() +
geom_bar(aes(x = y, fill = y))
# Randomly select a few variables to plot; focus on `y` in
# the first column and first row
trn %>%
select(y, X1, X7, X5)  %>%
ggpairs(
aes(color = trn$y),  # Color code is spam vs. not spam
lower = list(
combo = wrap(
"facethist",
binwidth = 0.5
)
)
)
# Stacked histograms of a few variables; note the sparse
# nature of text data
trn %>%
select(X1, X2, X3, X4, X5, X6, X7, X8, y) %>%
gather(var, val, -y) %>%  # Gather key value pairs
ggplot(aes(x = val, group = y, fill = y)) +
geom_histogram(binwidth = 1) +
facet_wrap(~var, ncol = 3) +
theme(legend.position = "bottom")
# SAVE DATA ################################################
# Use saveRDS(), which save data to native R formats
df  %>% saveRDS("data/ENB2012_data.rds")
trn %>% saveRDS("data/ENB2012_trn.rds")
tst %>% saveRDS("data/ENB2012_tst.rds")
# CLEAN UP #################################################
# Clear data
rm(list = ls())  # Removes all objects from the environment
# Clear packages
p_unload(all)    # Remove all contributed packages
# Clear plots
graphics.off()   # Clears plots, closes all graphics devices
# Clear console
cat("\014")      # Mimics ctrl+L
# Clear R
#   You may want to use Session > Restart R, as well, which
#   resets changed options, relative paths, dependencies,
#   and so on to let you start with a clean slate
# Clear mind :)
# Install and/or load packages with pacman
pacman::p_load(  # Use p_load function from pacman
caret,         # Train/test functions
e1071,         # Machine learning functions
magrittr,      # Pipes
pacman,        # Load/unload packages
rattle,        # Pretty plot for decision trees
rio,           # Import/export data
tidyverse,     # So many reasons
rpart,         # Fit a rpart model
rpart.plot,    # Plot an rpart model
pROC           # Tools for visualizing
)
# Set random seed to reproduce the results
set.seed(1)
# Import training data `trn`
trn <- import("data/ENB2012_trn.rds")
# Import testing data `tst`
tst <- import("data/ENB2012_tst.rds")
# set training control parameters
ctrlparam <- trainControl(
method  = "repeatedcv",   # method
number  = 5,              # 5 fold
repeats = 3               # 3 repeats
)
View(ctrlparam)
# set training control parameters
ctrlparam <- trainControl(
method  = "repeatedcv",   # method
number  = 5,              # 5 fold
repeats = 3               # 3 repeats
)
# Train decision tree on training data (takes a moment).
# First method tunes the complexity parameter.
dt1 <- train(
y ~ .,                  # Use all vars to predict spam
data = trn,             # Use training data
method = "rpart",       # Tune the complexity parameter
trControl = ctrlparam,  # Control parameters
tuneLength = 10         # Try ten parameters
)
# Show processing summary
dt1
# Plot accuracy by complexity parameter values
dt1 %>% plot()
dt1 %>% plot(ylim = c(0, 1))  # Plot with 0-100% range
# Second method tunes the maximum tree depth
dt2 <- train(
y ~ .,                  # Use all vars to predict spam
data = trn,             # Use training data
method = "rpart2",      # Tune the maximum tree depth
trControl = ctrlparam,  # Control parameters
tuneLength = 10         # Try ten parameters
)
# Show processing summary
dt2
# Plot the accuracy for different parameter values
dt2 %>% plot()
dt2 %>% plot(ylim = c(0, 1))  # Plot with 0-100% range
# Select the final model depending upon final accuracy
finaldt <- if (max(dt1$results$Accuracy) >
max(dt2$results$Accuracy)) {
dt1
} else {
dt2
}
# Description of final training model
finaldt$finalModel
# Plot the final decision tree model
finaldt$finalModel %>%
fancyRpartPlot(
main = "Predicting Heating_Load",
sub  = "Training Data"
)
# Predict on test set
pred <- finaldt %>%
predict(newdata = tst)
# Accuracy of model on test data
cmtest <- pred %>%
confusionMatrix(reference = tst$y)
# Plot the confusion matrix
cmtest$table %>%
fourfoldplot(color = c("red", "lightblue"))
# Print the confusion matrix
cmtest %>% print()
#############my code
### Step 3 - Fit a Decision Tree using training data
DTmodel1 <- rpart(y ~ .,method="class", data=trn, parms = list (split ="information gain"), control = rpart.control(minsplit = 10, maxdepth = 5))
DTmodel2 <- rpart(y ~ .,method="class", data=trn, parms = list (split ="gini"), control = rpart.control(minsplit = 15, maxdepth = 5))
# Fitting the model
rpart.plot(DTmodel1, type=3, extra = 101, fallen.leaves = F, cex = 0.8,box.palette="blue")
rpart.plot(DTmodel1,box.palette="blue")
rpart.plot(DTmodel2,box.palette="blue")
summary(DTmodel1) # detailed summary of splits
DTmodel1 #prints the rules
summary(DTmodel2) # detailed summary of splits
DTmodel2 #prints the rules
DTHLpredTest <- predict(DTmodel1, tst, type="class")
DTHLprobTest <- predict(DTmodel1, tst, type="prob")
DTHLactualTest <- tst$y
### Step 5 - Create Confusion Matrix and compute the misclassification error
DTHLt <- table(predictions= DTHLpredTest, actual = DTHLactualTest)
DTHLt # Confusion matrix
DTHLaccuracy <- sum(diag(DTHLt))/sum(DTHLt)
DTHLaccuracy
## Visualization of probabilities
hist(DTHLprobTest[,2], breaks = 100)
## ROC and Area Under the Curve
DTHLROC <- roc(DTHLactualTest, DTHLprobTest[,2])
plot(DTHLROC, col="blue")
DTHLAUC <- auc(DTHLROC)
DTHLAUC
#A new dataframe with Predicted Prob, Actual Value and Predicted Value
DTHLpredicted_data <- data.frame(Probs = DTHLprobTest, Actual_Value= DTHLactualTest ,Predicted_Value = DTHLpredTest )  #Create data frame with prob and predictions
DTHLpredicted_data <- DTHLpredicted_data[order(DTHLpredicted_data$Probs.1, decreasing=TRUE),] # Sort on Probabilities
DTHLpredicted_data <- DTHLpredicted_data[order(DTHLpredicted_data$Probs[1], decreasing=TRUE),] # Sort on Probabilities
DTHLpredicted_data <- DTHLpredicted_data[order(DTHLpredicted_data$Probs1, decreasing=TRUE),] # Sort on Probabilities
DTHLpredicted_data <- DTHLpredicted_data[order(DTHLpredicted_data$Probs, decreasing=TRUE),] # Sort on Probabilities
.
.
.
DTHLpredicted_data <- DTHLpredicted_data[order(DTHLpredicted_data$Probs.1, decreasing=TRUE),] # Sort on Probabilities
DTHLpredicted_data$Rank <- 1:nrow(DTHLpredicted_data) # Add a new variable rank
ggplot(data=DTpredicted_data, aes(x=Rank, y=Probs.1)) +
geom_point(aes(color = DTHLpredicted_data$Actual_Value)) + xlab("Index") + ylab("Predicted Probability of Heating Load")
rpart.plot(DTmodel1,box.palette="blue")
# Clear data
rm(list = ls())  # Removes all objects from the environment
# Clear packages
p_unload(all)    # Remove all contributed packages
# Clear plots
graphics.off()   # Clears plots, closes all graphics devices
# Clear console
cat("\014")      # Mimics ctrl+L
# Title:  Decision Trees
# File:   ENB2012_data.R
# Course: Knowledge Discovery and Data Analytics I - COMP 6115
# INSTALL AND LOAD PACKAGES ################################
# Install pacman if you don't have it (uncomment next line)
# install.packages("pacman")
# Install and/or load packages with pacman
pacman::p_load(  # Use p_load function from pacman
GGally,        # Plotting option
magrittr,      # Pipes
pacman,        # Load/unload packages
rio,           # Import/export data
tidyverse      # So many reasons
)
# LOAD AND PREPARE DATA ####################################
# Import the data
df <-  import("data/ENB2012_data.csv") %>%
as_tibble()  # Save as tibble, which prints better
# Look at the variable names
df %>% names()
head(df)
summary(df)
class(df)
str(df)
boxplot(df$Y1)
hist(df$Y1)
# Rename the class label as y; change values 0 to "notSpam"
# and 1 to "spam"; convert to factor
df %<>%
rename(y = Y1) %>%   # Rename class variable as `y`
mutate(
y = ifelse(
y >= 30,
"High",
"Low"
)
) %>%
mutate(y = factor(y))  # Recode class label as factor
# Check the variable `y`; `forcats::fct_count` gives
# frequencies in factor order
df %>%
pull(y) %>%  # Return a vector instead of a dataframe
fct_count()  # Count frequencies in factor order
# SPLIT DATA ##############################################
# Some demonstrations will use separate testing and training
# datasets for validation.
# Set random seed for reproducibility in processes like
# splitting the data
set.seed(1)  # You can use any number here
# Split data into training (trn) and testing (tst) sets
df %<>% mutate(ID = row_number())  # Add row ID
trn <- df %>%                      # Create trn
slice_sample(prop = .70)         # 70% in trn
tst <- df %>%                      # Create tst
anti_join(trn, by = "ID") %>%    # Remaining data in tst
select(-ID)                      # Remove id from tst
trn %<>% select(-ID)               # Remove id from trn
df %<>% select(-ID)                # Remove id from df
# EXPLORE TRAINING DATA ####################################
# Bar chart of `y`, which is the spam/not class variable
trn %>%
ggplot() +
geom_bar(aes(x = y, fill = y))
# Randomly select a few variables to plot; focus on `y` in
# the first column and first row
trn %>%
select(y, X1, X7, X5)  %>%
ggpairs(
aes(color = trn$y),  # Color code is spam vs. not spam
lower = list(
combo = wrap(
"facethist",
binwidth = 0.5
)
)
)
# Stacked histograms of a few variables; note the sparse
# nature of text data
trn %>%
select(X1, X2, X3, X4, X5, X6, X7, X8, y) %>%
gather(var, val, -y) %>%  # Gather key value pairs
ggplot(aes(x = val, group = y, fill = y)) +
geom_histogram(binwidth = 1) +
facet_wrap(~var, ncol = 3) +
theme(legend.position = "bottom")
# SAVE DATA ################################################
# Use saveRDS(), which save data to native R formats
df  %>% saveRDS("data/ENB2012_data.rds")
trn %>% saveRDS("data/ENB2012_trn.rds")
tst %>% saveRDS("data/ENB2012_tst.rds")
# CLEAN UP #################################################
# Clear data
rm(list = ls())  # Removes all objects from the environment
# Clear packages
p_unload(all)    # Remove all contributed packages
# Clear plots
graphics.off()   # Clears plots, closes all graphics devices
# Clear console
cat("\014")      # Mimics ctrl+L
# Clear R
#   You may want to use Session > Restart R, as well, which
#   resets changed options, relative paths, dependencies,
#   and so on to let you start with a clean slate
# Clear mind :)
# Install and/or load packages with pacman
pacman::p_load(  # Use p_load function from pacman
GGally,        # Plotting option
magrittr,      # Pipes
pacman,        # Load/unload packages
rio,           # Import/export data
tidyverse      # So many reasons
)
# Import the data
df <-  import("data/ENB2012_data.csv") %>%
as_tibble()  # Save as tibble, which prints better
# Look at the variable names
df %>% names()
# Rename the class label as y; change values 0 to "notSpam"
# and 1 to "spam"; convert to factor
df %<>%
rename(y = Y1) %>%   # Rename class variable as `y`
mutate(
y = ifelse(
y >= 30,
"High",
"Low"
)
) %>%
mutate(y = factor(y))  # Recode class label as factor
# Split data into training (trn) and testing (tst) sets
df %<>% mutate(ID = row_number())  # Add row ID
trn <- df %>%                      # Create trn
slice_sample(prop = .70)         # 70% in trn
tst <- df %>%                      # Create tst
anti_join(trn, by = "ID") %>%    # Remaining data in tst
select(-ID)                      # Remove id from tst
trn %<>% select(-ID)               # Remove id from trn
df %<>% select(-ID)                # Remove id from df
# Use saveRDS(), which save data to native R formats
df  %>% saveRDS("data/ENB2012_data.rds")
trn %>% saveRDS("data/ENB2012_trn.rds")
tst %>% saveRDS("data/ENB2012_tst.rds")
# Clear data
rm(list = ls())  # Removes all objects from the environment
# Clear packages
p_unload(all)    # Remove all contributed packages
# Clear plots
graphics.off()   # Clears plots, closes all graphics devices
# Clear console
cat("\014")      # Mimics ctrl+L
# Install and/or load packages with pacman
pacman::p_load(  # Use p_load function from pacman
caret,         # Train/test functions
e1071,         # Machine learning functions
magrittr,      # Pipes
pacman,        # Load/unload packages
rattle,        # Pretty plot for decision trees
rio,           # Import/export data
tidyverse,     # So many reasons
rpart,         # Fit a rpart model
rpart.plot,    # Plot an rpart model
pROC           # Tools for visualizing
)
# Set random seed to reproduce the results
set.seed(1)
# Import training data `trn`
trn <- import("data/ENB2012_trn.rds")
# Import testing data `tst`
tst <- import("data/ENB2012_tst.rds")
# set training control parameters
ctrlparam <- trainControl(
method  = "repeatedcv",   # method
number  = 5,              # 5 fold
repeats = 3               # 3 repeats
)
# Train decision tree on training data (takes a moment).
# First method tunes the complexity parameter.
dt1 <- train(
y ~ .,                  # Use all vars to predict spam
data = trn,             # Use training data
method = "rpart",       # Tune the complexity parameter
trControl = ctrlparam,  # Control parameters
tuneLength = 10         # Try ten parameters
)
# Show processing summary
dt1
# Second method tunes the maximum tree depth
dt2 <- train(
y ~ .,                  # Use all vars to predict spam
data = trn,             # Use training data
method = "rpart2",      # Tune the maximum tree depth
trControl = ctrlparam,  # Control parameters
tuneLength = 10         # Try ten parameters
)
# Show processing summary
dt2
# Select the final model depending upon final accuracy
finaldt <- if (max(dt1$results$Accuracy) >
max(dt2$results$Accuracy)) {
dt1
} else {
dt2
}
# Description of final training model
finaldt$finalModel
# Plot the final decision tree model
finaldt$finalModel %>%
fancyRpartPlot(
main = "Predicting Heating_Load",
sub  = "Training Data"
)
