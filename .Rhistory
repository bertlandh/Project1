str(dataset)
dataset$working_day <- as.factor(dataset$working_day)
dataset$weekend_day <- as.factor(dataset$weekend_day)
dataset$public_holiday <- as.factor(dataset$public_holiday)
str(dataset)
barplot(table(dataset$weekend_day), ylab ="Frequency", main = "Distribution of Target Class", col="lightblue")
barplot(table(dataset$working_day), ylab ="Frequency", main = "Distribution of Target Class", col="lightblue")
barplot(table(dataset$public_holiday), ylab ="Frequency", main = "Distribution of Target Class", col="lightblue")
hist(dataset$count)
dim(dataset)
head(dataset, 10)
is.na(dataset)
#number of row with at least one missing value
nrow(dataset[!complete.cases(dataset),])
#how many missing value in each ro/column
apply(dataset, 1,function(x) sum(is.na(x))) #number of NA per row
apply(dataset, 2,function(x) sum(is.na(x))) #number of NA per column
### Step 2 - Split data into training and testing data
set.seed(20)
wrkdDataset <-sample.split(Y=dataset$working_day, SplitRatio = 0.7)
wrkdtrainData <- dataset[wrkdDataset,]
dim(wrkdtrainData)
wrkdtestData <- dataset[!wrkdDataset,]
dim(wrkdtestData)
wekdDataset <-sample.split(Y=dataset$weekend_day, SplitRatio = 0.7)
wekdtrainData <- dataset[wekdDataset,]
dim(wekdtrainData)
wekdtestData <- dataset[!wekdDataset,]
dim(wekdtestData)
phdDataset <-sample.split(Y=dataset$public_holiday, SplitRatio = 0.7)
phdtrainData <- dataset[phdDataset,]
dim(phdtrainData)
phdtestData <- dataset[!phdDataset,]
dim(phdtestData)
### Step 3 - Fit a Logistic Model using training data
wrkdrModel <- glm(working_day ~ ., data=wrkdtrainData, family=binomial(link="logit"))
#Target Variable = Class, Input Vaiables = All, family = binomial (binary target variable) - Logistic regression using logit
summary(wrkdrModel)
wekdrModel <- glm(weekend_day ~ ., data=wekdtrainData, family=binomial(link="logit"))
#Target Variable = Class, Input Vaiables = All, family = binomial (binary target variable) - Logistic regression using logit
summary(wekdrModel)
phdrModel <- glm(public_holiday ~ ., data=phdtrainData, family=binomial(link="logit"))
#Target Variable = Class, Input Vaiables = All, family = binomial (binary target variable) - Logistic regression using logit
summary(phdrModel)
# log likelihood
logLik(wrkdrModel)
logLik(wekdrModel)
logLik(phdrModel)
# R-squared
wrkdll.null <- wrkdrModel$null.deviance/-2
wrkdll.proposed <- wrkdrModel$deviance/-2
wrkdr_sq <- (wrkdll.null - wrkdll.proposed)/wrkdll.null
wrkdr_sq
wekdll.null <- wekdrModel$null.deviance/-2
wekdll.proposed <- wekdrModel$deviance/-2
wekdr_sq <- (wekdll.null - wekdll.proposed)/wekdll.null
wekdr_sq
phdll.null <- phdrModel$null.deviance/-2
phdll.proposed <- phdrModel$deviance/-2
phdr_sq <- (phdll.null - phdll.proposed)/phdll.null
phdr_sq
#P value
wrkdp_value <- 1 - pchisq(2*(wrkdll.proposed - wrkdll.null), df = (length(wrkdrModel$coefficients)-1))
wrkdp_value
wekdp_value <- 1 - pchisq(2*(wekdll.proposed - wekdll.null), df = (length(wekdrModel$coefficients)-1))
wekdp_value
phdp_value <- 1 - pchisq(2*(phdll.proposed - phdll.null), df = (length(phdrModel$coefficients)-1))
phdp_value
###Step 4 - Use the fitted model to do predictions for the test data
#probTest- Probability for Test data, predTest - Predictions for Test data, actual - Actual Value in test data
wrkdprobTest=predict(wrkdrModel, wrkdtestData, type = "response")  # Predict probabilities
wekdprobTest=predict(wekdrModel, wekdtestData, type = "response")  # Predict probabilities
phdprobTest=predict(phdrModel, phdtestData, type = "response")  # Predict probabilities
#Recode probability to classification
wrkdpredVal <- ifelse(wrkdprobTest >= 0.5, 1, 0)
wrkdpredTest <- factor(wrkdpredVal, levels = c(0,1))
wekdpredVal <- ifelse(wekdprobTest >= 0.5, 1, 0)
wekdpredTest <- factor(wekdpredVal, levels = c(0,1))
phdpredVal <- ifelse(phdprobTest >= 0.5, 1, 0)
phdpredTest <- factor(phdpredVal, levels = c(0,1))
wrkdprobTest [0:5]
wrkdpredTest[0:5]
wekdprobTest [0:5]
wekdpredTest[0:5]
phdprobTest [0:5]
phdpredTest[0:5]
wrkdactualTest <-wrkdtestData$working_day
wrkdactualTest[0:5]
wekdactualTest <-wekdtestData$weekend_day
wekdactualTest[0:5]
phdactualTest <-phdtestData$public_holiday
phdactualTest[0:5]
### Step 5 - Create Confusion Matrix and compute the misclassification error
wrkdt <- table(predictions=wrkdpredTest, actual = wrkdactualTest)
wrkdt # Confusion matrix
wrkdaccuracy <- sum(diag(wrkdt))/sum(wrkdt)
wrkdaccuracy
wekdt <- table(predictions=wekdpredTest, actual = wekdactualTest)
wekdt # Confusion matrix
wekdaccuracy <- sum(diag(wekdt))/sum(wekdt)
wekdaccuracy
phdt <- table(predictions=phdpredTest, actual = phdactualTest)
phdt # Confusion matrix
phdaccuracy <- sum(diag(phdt))/sum(phdt)
phdaccuracy
### ROC and Area Under the Curve
wrkdROC1 <- roc(wrkdactualTest, wrkdprobTest)
plot(wrkdROC1, col="blue")
wrkdAUC1 <- auc(wrkdROC1)
wrkdAUC1
wekdROC1 <- roc(wekdactualTest, wekdprobTest)
plot(wekdROC1, col="blue")
wekdAUC1 <- auc(wekdROC1)
wekdAUC1
phdROC1 <- roc(phdactualTest, phdprobTest)
plot(phdROC1, col="blue")
phdAUC1 <- auc(phdROC1)
phdAUC1
#A new dataframe with Predicted Prob, Actual Value and Predicted Value
wrkdpredicted_data <- data.frame(Probs = wrkdprobTest, Actual_Value=wrkdactualTest,Predicted_Value = wrkdpredTest )  #Create data frame with prob and predictions
wrkdpredicted_data <- wrkdpredicted_data[order(wrkdpredicted_data$Probs, decreasing=TRUE),] # Sort on Probabilities
wrkdpredicted_data$Rank <- 1:nrow(wrkdpredicted_data) # Add a new variable rank
wekdpredicted_data <- data.frame(Probs = wekdprobTest, Actual_Value=wekdactualTest,Predicted_Value = wekdpredTest )  #Create data frame with prob and predictions
wekdpredicted_data <- wekdpredicted_data[order(wekdpredicted_data$Probs, decreasing=TRUE),] # Sort on Probabilities
wekdpredicted_data$Rank <- 1:nrow(wekdpredicted_data) # Add a new variable rank
phdpredicted_data <- data.frame(Probs = phdprobTest, Actual_Value=phdactualTest,Predicted_Value = phdpredTest )  #Create data frame with prob and predictions
phdpredicted_data <- phdpredicted_data[order(phdpredicted_data$Probs, decreasing=TRUE),] # Sort on Probabilities
phdpredicted_data$Rank <- 1:nrow(phdpredicted_data) # Add a new variable rank
View(phdpredicted_data)
rm(list = ls())
setwd("C:/Data/BujuBanton/msc/comp6115kdda/worksheets/Project1")
getwd()
rm(list = ls())
options(scipen = 99999)
#install needed libraries
#installus <- c("rpart","raprt.plot","pROC","caTools","keras")
#install.packages(installus)
#library(caTools)
#library(pROC)
#library(keras)
#library(dplyr)
### Step 1 - Load data and get summaries
dataset <- read.csv("framingham.csv")
summary(dataset)
str(dataset)
#how many missing value in each ro/column
apply(dataset, 2,function(x) sum(is.na(x))) #number of NA per column
## Replacing null values with the mean value
dataset[is.na(dataset$education),"education"] <- mean(dataset$education,na.rm = T)
dataset[is.na(dataset$cigsPerDay),"cigsPerDay"] <- mean(dataset$cigsPerDay, na.rm = T)
dataset[is.na(dataset$BPMeds),"BPMeds"] <- mean(dataset$BPMeds,na.rm = T)
dataset[is.na(dataset$totChol),"totChol"] <- mean(dataset$totChol,na.rm = T)
dataset[is.na(dataset$BMI),"BMI"] <- mean(dataset$BMI,na.rm = T)
dataset[is.na(dataset$heartRate),"heartRate"] <- mean(dataset$heartRate,na.rm = T)
dataset[is.na(dataset$glucose),"glucose"] <- mean(dataset$glucose,na.rm = T)
#dataset$education <- NULL
#dataset$currentSmoker <- NULL
#dataset$BPMeds <- NULL
#dataset$prevalentStroke <- NULL
#dataset$prevalentHyp <- NULL
#dataset$diabetes <- NULL
#dataset$totChol <- NULL
#dataset$diaBP <- NULL
#dataset$BMI <- NULL
#dataset$heartRate <- NULL
# target variable dependent variable to factor
dataset$TenYearCHD <- as.factor(dataset$TenYearCHD)
## Rechecking for null values
apply(dataset, 2,function(x) sum(is.na(x))) #number of NA per column
dim(dataset)
head(dataset, 10)
barplot(table(dataset$TenYearCHD), ylab ="Frequency", main = "Distribution of Target Class", col="lightblue")
hist(dataset$age)
## CHD value counts
table(dataset$TenYearCHD)
### Step 2 - Split data into training and testing data
set.seed(20)
LRDataset <-sample.split(Y=dataset$TenYearCHD, SplitRatio = 0.7)
LRtrainData <- dataset[LRDataset,]
dim(LRtrainData)
LRtestData <- dataset[!LRDataset,]
dim(LRtestData)
### Step 3 - Fit a Logistic Model using training data
#Target Variable = TenYearCHD, Input Vaiables = All, family = binomial (binary target variable) - Logistic regression using logit
LRModel <- glm(TenYearCHD ~ ., data=LRtrainData, family=binomial(link="logit"))
summary(LRModel)
# log likelihood
logLik(LRModel)
# R-squared
LRll.null <- LRModel$null.deviance/-2
LRll.proposed <- LRModel$deviance/-2
LRr_sq <- (LRll.null - LRll.proposed)/LRll.null
LRr_sq
#P value
LRp_value <- 1 - pchisq(2*(LRll.proposed - LRll.null), df = (length(LRModel$coefficients)-1))
LRp_value
###Step 4 - Use the fitted model to do predictions for the test data
#LRprobTest- Probability for Test data, LRpredTest - Predictions for Test data, actual - Actual Value in test data
LRprobTest=predict(LRModel, LRtestData, type = "response")  # Predict probabilities
LRprobTest
LRtestData$LRprobTest <- LRpredTest
#Recode probability to classification
LRpredVal <- ifelse(LRprobTest >= 0.5, 1, 0)
LRpredTest <- factor(LRpredVal, levels = c(0,1))
#LRprobTest [0:45]
#LRpredTest[0:45]
LRprobTest
LRpredTest
LRactualTest <-LRtestData$TenYearCHD
#LRactualTest[0:5]
LRactualTest
### Step 5 - Create Confusion Matrix and compute the misclassification error
LRt <- table(predictions=LRpredTest, actual = LRactualTest)
LRt # Confusion matrix
LRaccuracy <- sum(diag(LRt))/sum(LRt)
LRaccuracy
### ROC and Area Under the Curve
LRROC1 <- roc(LRactualTest, LRprobTest)
plot(LRROC1, col="blue")
LRAUC1 <- auc(LRROC1)
LRAUC1
#A new dataframe with Predicted Prob, Actual Value and Predicted Value
LRpredicted_data <- data.frame(Probs = LRprobTest, Actual_Value=LRactualTest,Predicted_Value = LRpredTest )  #Create data frame with prob and predictions
LRpredicted_data <- LRpredicted_data[order(LRpredicted_data$Probs, decreasing=TRUE),] # Sort on Probabilities
LRpredicted_data$Rank <- 1:nrow(LRpredicted_data) # Add a new variable rank
library(ggplot2)
ggplot(data=LRpredicted_data, aes(x=Rank, y=Probs)) +
geom_point(aes(color = LRpredicted_data$Actual_Value)) + xlab("Index") + ylab("Heart Disease Prediction")
### Step 6 - Use model to make predictions on newdata. Note we can specify the newData as data.frame with one or many records
# male = 1, age = 1, education = 1, currentSmoker = 1, cigsPerDay = 1, BPMeds = 1, prevalentStroke = 1, prevalentHyp = 1, diabetes = 1, totChol = 1, sysBP = 1, diaBP = 1, BMI = 1, heartRate = 1, glucose = 1 TenYearCHD
#newData <- data.frame(male = 1, age = 47, cigsPerDay = 9.003089, sysBP = 102.0, glucose = 81.96675 )
#newData <- data.frame(male = 0, age = 61, cigsPerDay = 30, sysBP = 150.0, glucose = 103 )
#newData <- data.frame(male = 1, age = 47, cigsPerDay = 20, sysBP = 102.0, glucose = 66 )
newData <- data.frame(male = 1, age = 47, education = 1.97895, currentSmoker = 1, cigsPerDay = 9.003089, BPMeds = 0.02962963, prevalentStroke = 0, prevalentHyp = 0, diabetes = 0, totChol = 236.7216, sysBP = 102.0, diaBP = 68.0, BMI = 25.80201, heartRate = 75.87892, glucose = 81.96675 )
predProbability <-predict(LRModel, newData, type='response')
predProbability
## Performance measures
# Simplicity =  coefficients
# Accuracy = 0.8528718 0r 0.85
# AUC = 0.7071 0r 0.71
View(newData)
setwd("C:/Data/BujuBanton/msc/comp6115kdda/worksheets/Project1")
getwd()
rm(list = ls())
options(scipen = 99999)
#install needed libraries
#installus <- c("rpart","raprt.plot","pROC","caTools","keras")
#install.packages(installus)
#library(caTools)
#library(pROC)
#library(keras)
#library(dplyr)
### Step 1 - Load data and get summaries
dataset <- read.csv("framingham.csv")
summary(dataset)
str(dataset)
#how many missing value in each ro/column
apply(dataset, 2,function(x) sum(is.na(x))) #number of NA per column
## Replacing null values with the mean value
dataset[is.na(dataset$education),"education"] <- mean(dataset$education,na.rm = T)
dataset[is.na(dataset$cigsPerDay),"cigsPerDay"] <- mean(dataset$cigsPerDay, na.rm = T)
dataset[is.na(dataset$BPMeds),"BPMeds"] <- mean(dataset$BPMeds,na.rm = T)
dataset[is.na(dataset$totChol),"totChol"] <- mean(dataset$totChol,na.rm = T)
dataset[is.na(dataset$BMI),"BMI"] <- mean(dataset$BMI,na.rm = T)
dataset[is.na(dataset$heartRate),"heartRate"] <- mean(dataset$heartRate,na.rm = T)
dataset[is.na(dataset$glucose),"glucose"] <- mean(dataset$glucose,na.rm = T)
#dataset$education <- NULL
#dataset$currentSmoker <- NULL
#dataset$BPMeds <- NULL
#dataset$prevalentStroke <- NULL
#dataset$prevalentHyp <- NULL
#dataset$diabetes <- NULL
#dataset$totChol <- NULL
#dataset$diaBP <- NULL
#dataset$BMI <- NULL
#dataset$heartRate <- NULL
# target variable dependent variable to factor
dataset$TenYearCHD <- as.factor(dataset$TenYearCHD)
## Rechecking for null values
apply(dataset, 2,function(x) sum(is.na(x))) #number of NA per column
dim(dataset)
head(dataset, 10)
barplot(table(dataset$TenYearCHD), ylab ="Frequency", main = "Distribution of Target Class", col="lightblue")
hist(dataset$age)
## CHD value counts
table(dataset$TenYearCHD)
### Step 2 - Split data into training and testing data
set.seed(20)
LRDataset <-sample.split(Y=dataset$TenYearCHD, SplitRatio = 0.7)
LRtrainData <- dataset[LRDataset,]
dim(LRtrainData)
LRtestData <- dataset[!LRDataset,]
dim(LRtestData)
### Step 3 - Fit a Logistic Model using training data
#Target Variable = TenYearCHD, Input Vaiables = All, family = binomial (binary target variable) - Logistic regression using logit
LRModel <- glm(TenYearCHD ~ ., data=LRtrainData, family=binomial(link="logit"))
summary(LRModel)
# log likelihood
logLik(LRModel)
# R-squared
LRll.null <- LRModel$null.deviance/-2
LRll.proposed <- LRModel$deviance/-2
LRr_sq <- (LRll.null - LRll.proposed)/LRll.null
LRr_sq
#P value
LRp_value <- 1 - pchisq(2*(LRll.proposed - LRll.null), df = (length(LRModel$coefficients)-1))
LRp_value
###Step 4 - Use the fitted model to do predictions for the test data
#LRprobTest- Probability for Test data, LRpredTest - Predictions for Test data, actual - Actual Value in test data
LRprobTest=predict(LRModel, LRtestData, type = "response")  # Predict probabilities
LRprobTest
LRtestData$LRprobTest <- LRpredTest
#Recode probability to classification
LRpredVal <- ifelse(LRprobTest >= 0.5, 1, 0)
LRpredTest <- factor(LRpredVal, levels = c(0,1))
#LRprobTest [0:45]
#LRpredTest[0:45]
LRprobTest
LRpredTest
LRactualTest <-LRtestData$TenYearCHD
#LRactualTest[0:5]
LRactualTest
### Step 5 - Create Confusion Matrix and compute the misclassification error
LRt <- table(predictions=LRpredTest, actual = LRactualTest)
LRt # Confusion matrix
LRaccuracy <- sum(diag(LRt))/sum(LRt)
LRaccuracy
### ROC and Area Under the Curve
LRROC1 <- roc(LRactualTest, LRprobTest)
plot(LRROC1, col="blue")
LRAUC1 <- auc(LRROC1)
LRAUC1
#A new dataframe with Predicted Prob, Actual Value and Predicted Value
LRpredicted_data <- data.frame(Probs = LRprobTest, Actual_Value=LRactualTest,Predicted_Value = LRpredTest )  #Create data frame with prob and predictions
LRpredicted_data <- LRpredicted_data[order(LRpredicted_data$Probs, decreasing=TRUE),] # Sort on Probabilities
LRpredicted_data$Rank <- 1:nrow(LRpredicted_data) # Add a new variable rank
library(ggplot2)
ggplot(data=LRpredicted_data, aes(x=Rank, y=Probs)) +
geom_point(aes(color = LRpredicted_data$Actual_Value)) + xlab("Index") + ylab("Heart Disease Prediction")
### Step 6 - Use model to make predictions on newdata. Note we can specify the newData as data.frame with one or many records
# male = 1, age = 1, education = 1, currentSmoker = 1, cigsPerDay = 1, BPMeds = 1, prevalentStroke = 1, prevalentHyp = 1, diabetes = 1, totChol = 1, sysBP = 1, diaBP = 1, BMI = 1, heartRate = 1, glucose = 1 TenYearCHD
#newData <- data.frame(male = 1, age = 47, cigsPerDay = 9.003089, sysBP = 102.0, glucose = 81.96675 )
#newData <- data.frame(male = 0, age = 61, cigsPerDay = 30, sysBP = 150.0, glucose = 103 )
#newData <- data.frame(male = 1, age = 47, cigsPerDay = 20, sysBP = 102.0, glucose = 66 )
newData <- data.frame(male = 1, age = 47, education = 1.97895, currentSmoker = 1, cigsPerDay = 9.003089, BPMeds = 0.02962963, prevalentStroke = 0, prevalentHyp = 0, diabetes = 0, totChol = 236.7216, sysBP = 102.0, diaBP = 68.0, BMI = 25.80201, heartRate = 75.87892, glucose = 81.96675 )
predProbability <-predict(LRModel, newData, type='response')
predProbability
## Performance measures
# Simplicity =  coefficients
# Accuracy = 0.8528718 0r 0.85
# AUC = 0.7071 0r 0.71
dataset[is.na(dataset$education),"education"] <- mean(dataset$education,na.rm = T)
dataset[is.na(dataset$cigsPerDay),"cigsPerDay"] <- mean(dataset$cigsPerDay, na.rm = T)
dataset[is.na(dataset$BPMeds),"BPMeds"] <- mean(dataset$BPMeds,na.rm = T)
dataset[is.na(dataset$totChol),"totChol"] <- mean(dataset$totChol,na.rm = T)
dataset[is.na(dataset$BMI),"BMI"] <- mean(dataset$BMI,na.rm = T)
dataset[is.na(dataset$heartRate),"heartRate"] <- mean(dataset$heartRate,na.rm = T)
dataset[is.na(dataset$glucose),"glucose"] <- mean(dataset$glucose,na.rm = T)
# target variable dependent variable to factor
dataset$TenYearCHD <- as.factor(dataset$TenYearCHD)
## Rechecking for null values
apply(dataset, 2,function(x) sum(is.na(x))) #number of NA per column
dim(dataset)
head(dataset, 10)
barplot(table(dataset$TenYearCHD), ylab ="Frequency", main = "Distribution of Target Class", col="lightblue")
hist(dataset$age)
## CHD value counts
table(dataset$TenYearCHD)
### Step 2 - Split data into training and testing data
set.seed(20)
LRDataset <-sample.split(Y=dataset$TenYearCHD, SplitRatio = 0.7)
#library(caTools)
#library(pROC)
library(keras)
### Step 2 - Split data into training and testing data
set.seed(20)
LRDataset <-sample.split(Y=dataset$TenYearCHD, SplitRatio = 0.7)
#library(caTools)
library(pROC)
### Step 2 - Split data into training and testing data
set.seed(20)
LRDataset <-sample.split(Y=dataset$TenYearCHD, SplitRatio = 0.7)
library(caTools)
### Step 2 - Split data into training and testing data
set.seed(20)
LRDataset <-sample.split(Y=dataset$TenYearCHD, SplitRatio = 0.7)
rm(list = ls())
library(caTools)
dataset <- read.csv("framingham.csv")
summary(dataset)
str(dataset)
apply(dataset, 2,function(x) sum(is.na(x))) #number of NA per column
## Replacing null values with the mean value
dataset[is.na(dataset$education),"education"] <- mean(dataset$education,na.rm = T)
dataset[is.na(dataset$cigsPerDay),"cigsPerDay"] <- mean(dataset$cigsPerDay, na.rm = T)
dataset[is.na(dataset$BPMeds),"BPMeds"] <- mean(dataset$BPMeds,na.rm = T)
dataset[is.na(dataset$totChol),"totChol"] <- mean(dataset$totChol,na.rm = T)
dataset[is.na(dataset$BMI),"BMI"] <- mean(dataset$BMI,na.rm = T)
dataset[is.na(dataset$heartRate),"heartRate"] <- mean(dataset$heartRate,na.rm = T)
dataset[is.na(dataset$glucose),"glucose"] <- mean(dataset$glucose,na.rm = T)
# target variable dependent variable to factor
dataset$TenYearCHD <- as.factor(dataset$TenYearCHD)
## Rechecking for null values
apply(dataset, 2,function(x) sum(is.na(x))) #number of NA per column
dim(dataset)
head(dataset, 10)
barplot(table(dataset$TenYearCHD), ylab ="Frequency", main = "Distribution of Target Class", col="lightblue")
hist(dataset$age)
## CHD value counts
table(dataset$TenYearCHD)
### Step 2 - Split data into training and testing data
set.seed(20)
LRDataset <-sample.split(Y=dataset$TenYearCHD, SplitRatio = 0.7)
LRtrainData <- dataset[LRDataset,]
dim(LRtrainData)
LRtestData <- dataset[!LRDataset,]
dim(LRtestData)
### Step 3 - Fit a Logistic Model using training data
#Target Variable = TenYearCHD, Input Vaiables = All, family = binomial (binary target variable) - Logistic regression using logit
LRModel <- glm(TenYearCHD ~ ., data=LRtrainData, family=binomial(link="logit"))
summary(LRModel)
### Step 3 - Fit a Logistic Model using training data
#Target Variable = TenYearCHD, Input Vaiables = All, family = binomial (binary target variable) - Logistic regression using logit
LRModel <- glm(TenYearCHD ~ ., data=LRtrainData, family=binomial(link="logit"))
summary(LRModel)
LRt # Confusion matrix
setwd("C:/Data/BujuBanton/msc/comp6115kdda/worksheets/Project1")
getwd()
rm(list = ls())
options(scipen = 99999)
#install needed libraries
#installus <- c("rpart","raprt.plot","pROC","caTools","keras")
#install.packages(installus)
library(caTools)
#library(pROC)
#library(keras)
#library(dplyr)
### Step 1 - Load data and get summaries
dataset <- read.csv("framingham.csv")
summary(dataset)
str(dataset)
#how many missing value in each ro/column
apply(dataset, 2,function(x) sum(is.na(x))) #number of NA per column
## Replacing null values with the mean value
dataset[is.na(dataset$education),"education"] <- mean(dataset$education,na.rm = T)
dataset[is.na(dataset$cigsPerDay),"cigsPerDay"] <- mean(dataset$cigsPerDay, na.rm = T)
dataset[is.na(dataset$BPMeds),"BPMeds"] <- mean(dataset$BPMeds,na.rm = T)
dataset[is.na(dataset$totChol),"totChol"] <- mean(dataset$totChol,na.rm = T)
dataset[is.na(dataset$BMI),"BMI"] <- mean(dataset$BMI,na.rm = T)
dataset[is.na(dataset$heartRate),"heartRate"] <- mean(dataset$heartRate,na.rm = T)
dataset[is.na(dataset$glucose),"glucose"] <- mean(dataset$glucose,na.rm = T)
#dataset$education <- NULL
#dataset$currentSmoker <- NULL
#dataset$BPMeds <- NULL
#dataset$prevalentStroke <- NULL
#dataset$prevalentHyp <- NULL
#dataset$diabetes <- NULL
#dataset$totChol <- NULL
#dataset$diaBP <- NULL
#dataset$BMI <- NULL
#dataset$heartRate <- NULL
# target variable dependent variable to factor
dataset$TenYearCHD <- as.factor(dataset$TenYearCHD)
## Rechecking for null values
apply(dataset, 2,function(x) sum(is.na(x))) #number of NA per column
dim(dataset)
head(dataset, 10)
barplot(table(dataset$TenYearCHD), ylab ="Frequency", main = "Distribution of Target Class", col="lightblue")
hist(dataset$age)
## CHD value counts
table(dataset$TenYearCHD)
### Step 2 - Split data into training and testing data
set.seed(20)
LRDataset <-sample.split(Y=dataset$TenYearCHD, SplitRatio = 0.7)
LRtrainData <- dataset[LRDataset,]
dim(LRtrainData)
LRtestData <- dataset[!LRDataset,]
dim(LRtestData)
### Step 3 - Fit a Logistic Model using training data
#Target Variable = TenYearCHD, Input Vaiables = All, family = binomial (binary target variable) - Logistic regression using logit
LRModel <- glm(TenYearCHD ~ ., data=LRtrainData, family=binomial(link="logit"))
summary(LRModel)
# log likelihood
logLik(LRModel)
# R-squared
LRll.null <- LRModel$null.deviance/-2
LRll.proposed <- LRModel$deviance/-2
LRr_sq <- (LRll.null - LRll.proposed)/LRll.null
LRr_sq
#P value
LRp_value <- 1 - pchisq(2*(LRll.proposed - LRll.null), df = (length(LRModel$coefficients)-1))
LRp_value
###Step 4 - Use the fitted model to do predictions for the test data
#LRprobTest- Probability for Test data, LRpredTest - Predictions for Test data, actual - Actual Value in test data
LRprobTest=predict(LRModel, LRtestData, type = "response")  # Predict probabilities
LRprobTest
LRtestData$LRprobTest <- LRpredTest
#Recode probability to classification
LRpredVal <- ifelse(LRprobTest >= 0.5, 1, 0)
LRpredTest <- factor(LRpredVal, levels = c(0,1))
#LRprobTest [0:45]
#LRpredTest[0:45]
LRprobTest
LRpredTest
LRactualTest <-LRtestData$TenYearCHD
#LRactualTest[0:5]
LRactualTest
### Step 5 - Create Confusion Matrix and compute the misclassification error
LRt <- table(predictions=LRpredTest, actual = LRactualTest)
LRt # Confusion matrix
LRt # Confusion matrix
hist(LRt)
